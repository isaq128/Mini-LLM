ğŸš€ MiniLLM
Transformer Language Model Built From Scratch (PyTorch)
ğŸ§  Overview
MiniLLM is a character-level GPT-style Transformer
implemented entirely from scratch using PyTorch.

No Hugging Face.
No pre-built Transformer modules.
All attention mechanisms implemented manually.

MiniLLM learns to model:

P(next_token | previous_tokens)

It predicts the next character using a decoder-only Transformer architecture.

ğŸ— Architecture
Input Tokens
     â†“
Token Embedding
     â†“
Positional Embedding
     â†“
Transformer Block Ã— 2
     â”œâ”€â”€ Multi-Head Self-Attention
     â”œâ”€â”€ Feedforward Network
     â”œâ”€â”€ Residual Connections
     â””â”€â”€ LayerNorm
     â†“
Linear Output Layer
     â†“
Softmax
ğŸ”¬ Model Configuration
Embedding Dimension  : 128
Transformer Blocks   : 2
Attention Heads      : 4
Context Window       : 64 tokens
Dropout              : 0.2
Optimizer            : AdamW
Learning Rate        : 3e-4
ğŸ“‰ Training
Dataset        : Shakespeare Corpus
Training Steps : 4000
Batch Size     : 32
Final Val Loss : ~1.9

Model shows stable convergence and meaningful structure learning.

ğŸ” Generation Strategy
1. Feed prompt
2. Compute logits
3. Apply temperature scaling
4. Apply Top-k filtering
5. Sample next token
6. Repeat

Supports:

âœ” Temperature control
âœ” Top-k sampling
âœ” Autoregressive decoding

â–¶ï¸ Run

Install

pip install torch

Train

python train.py

Infer

python infer.py
ğŸ¯ What This Demonstrates
âœ” Transformer implementation from scratch
âœ” Multi-head self-attention mechanics
âœ” Residual + LayerNorm structure
âœ” Autoregressive language modeling
âœ” Practical training pipeline
ğŸš€ Future Improvements
â€¢ BPE Tokenization
â€¢ Larger model scaling
â€¢ Web deployment
â€¢ Domain fine-tuning
â€¢ Perplexity tracking
